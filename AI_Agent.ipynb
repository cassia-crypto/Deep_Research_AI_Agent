{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Installing Libraries\n"
      ],
      "metadata": {
        "id": "l9r1012QEDOt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95rnX-RltRS-"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langgraph tavily-python openai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community langchain\n"
      ],
      "metadata": {
        "id": "ywPD2OZJt7pB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Tavily API key (sign up at https://app.tavily.com/)\n",
        "os.environ[\"TAVILY_API_KEY\"] = \"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\"\n"
      ],
      "metadata": {
        "id": "AjJ6KhCOtn-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Research Agent\n"
      ],
      "metadata": {
        "id": "6MYzKyM4EZKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tavily import TavilyClient\n",
        "\n",
        "# Initialize the client with your API Key\n",
        "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
        "\n",
        "def research_agent(query):\n",
        "    # Perform a search\n",
        "    search_results = tavily.search(\n",
        "        query=query,\n",
        "        search_depth=\"advanced\",   # you can set it to \"basic\" for faster response\n",
        "        include_raw_content=True,\n",
        "        max_results=5\n",
        "    )\n",
        "\n",
        "    # Extract content\n",
        "    collected_data = []\n",
        "    for result in search_results['results']:\n",
        "        collected_data.append(result['content'])\n",
        "\n",
        "    return \" \".join(collected_data)\n"
      ],
      "metadata": {
        "id": "jm5zbjNwEeD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Drafting an Answer\n"
      ],
      "metadata": {
        "id": "Bl7UJ_GGElzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def drafting_agent(research_content, user_query):\n",
        "    # Very basic draft logic\n",
        "    answer = f\"Based on the research conducted for: '{user_query}', the answer is summarized as:\\n\\n{research_content[:1000]}...\"\n",
        "    return answer\n"
      ],
      "metadata": {
        "id": "gVW0Qmi7Eg7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect using LangGraph"
      ],
      "metadata": {
        "id": "H5heGYM2EtDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph\n"
      ],
      "metadata": {
        "id": "bBfAIvVoFKIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import langgraph\n",
        "dir(langgraph)\n"
      ],
      "metadata": {
        "id": "D9ZmbbazFYx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph import graph\n",
        "dir(graph)\n"
      ],
      "metadata": {
        "id": "gduFKIJdFeQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "from langgraph.graph import StateGraph\n",
        "from typing import TypedDict, Optional\n",
        "\n",
        "# Define the state schema for the Research Agent\n",
        "class ResearchState(TypedDict):\n",
        "    query: str\n",
        "    research_content: Optional[str]\n",
        "    final_answer: Optional[str]\n",
        "\n",
        "# Initialize the graph - Assuming StateGraph is used\n",
        "graph = StateGraph(state_schema=ResearchState)\n",
        "\n",
        "# Define the research agent function with date handling\n",
        "def research_agent(query: str) -> str:\n",
        "    # If the query is related to the date, return the current date\n",
        "    if \"today's date\" in query.lower():\n",
        "        return f\"Today's date is: {datetime.datetime.now().strftime('%Y-%m-%d')}\"\n",
        "    # If the query is not recognized, return a generic message\n",
        "    return f\"Research results for query: {query}\"\n",
        "\n",
        "# Define the drafting agent function (drafts the final answer)\n",
        "def drafting_agent(research_content: Optional[str], query: str) -> str:\n",
        "    # If research content exists, create the final answer\n",
        "    if research_content:\n",
        "        return f\"Final answer for '{query}': {research_content}\"\n",
        "    return \"No research content available\"\n",
        "\n",
        "# Define the search node function\n",
        "def search_node(state: ResearchState) -> ResearchState:\n",
        "    research_content = research_agent(state[\"query\"])  # Perform research\n",
        "    state[\"research_content\"] = research_content  # Add research to state\n",
        "    return state\n",
        "\n",
        "# Define the draft node function\n",
        "def draft_node(state: ResearchState) -> ResearchState:\n",
        "    final_answer = drafting_agent(state[\"research_content\"], state[\"query\"])  # Generate final answer\n",
        "    state[\"final_answer\"] = final_answer  # Add final answer to state\n",
        "    return state\n",
        "\n",
        "# Add the search and draft nodes to the graph using unique identifiers\n",
        "graph.add_node(\"search\", search_node)  # Add search node with identifier\n",
        "graph.add_node(\"draft\", draft_node)    # Add draft node with identifier\n",
        "\n",
        "# Manually set the nodes and add edges between them using identifiers\n",
        "graph.set_entry_point(\"search\")  # Set search node as entry point\n",
        "graph.add_edge(\"search\", \"draft\")  # Connect search and draft nodes using their identifiers\n",
        "\n",
        "# Compile the system into an executable process\n",
        "deep_research_system = graph.compile()\n",
        "\n",
        "# Test the system with user input\n",
        "user_query = input(\"Enter your research query: \")\n",
        "state = {\"query\": user_query, \"research_content\": None, \"final_answer\": None}\n",
        "final_state = deep_research_system.invoke(state)\n",
        "\n",
        "# Output the final drafted answer\n",
        "print(\"\\n\\nFinal Answer:\\n\")\n",
        "print(final_state[\"final_answer\"])\n"
      ],
      "metadata": {
        "id": "x_SIPOOlEr8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Research Agent"
      ],
      "metadata": {
        "id": "yLYfRnG6HqXI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tavily import TavilyClient\n",
        "\n",
        "# Initialize Tavily client\n",
        "tavily = TavilyClient(api_key=\"YOUR_TAVILY_API_KEY\")  # Replace YOUR_TAVILY_API_KEY\n",
        "\n",
        "def research_agent(query: str) -> str:\n",
        "    # Search the web using Tavily\n",
        "    response = tavily.search(query=query, max_results=3)  # You can choose max_results\n",
        "    results = response.get('results', [])\n",
        "\n",
        "    if not results:\n",
        "        return \"No research content found.\"\n",
        "\n",
        "    # Combine the top results into a research content string\n",
        "    research_summary = \"\\n\\n\".join([f\"{res['title']}: {res['url']}\" for res in results])\n",
        "    return research_summary\n"
      ],
      "metadata": {
        "id": "FvE-HLJ2Hsod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Drafting Agent"
      ],
      "metadata": {
        "id": "5pcqeJ_xKzyb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def drafting_agent(research_content: Optional[str], query: str) -> str:\n",
        "    if research_content:\n",
        "        return f\"Here is the research for '{query}':\\n\\n{research_content}\"\n",
        "    return \"No research content available.\"\n"
      ],
      "metadata": {
        "id": "unUNextlHvQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Final Deep Research System"
      ],
      "metadata": {
        "id": "4fiZg6WzIYvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Tavily if you haven't\n",
        "!pip install tavily-python\n",
        "\n",
        "# Imports\n",
        "from langgraph.graph import StateGraph\n",
        "from typing import TypedDict, Optional\n",
        "from tavily import TavilyClient\n",
        "\n",
        "# Define your state schema\n",
        "class ResearchState(TypedDict):\n",
        "    query: str\n",
        "    research_content: Optional[str]\n",
        "    final_answer: Optional[str]\n",
        "\n",
        "# Initialize the StateGraph\n",
        "graph = StateGraph(state_schema=ResearchState)\n",
        "\n",
        "# Initialize Tavily client\n",
        "import os\n",
        "tavily = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
        "\n",
        "\n",
        "# Define the research agent function using Tavily\n",
        "def research_agent(query: str) -> str:\n",
        "    response = tavily.search(query=query, max_results=3)\n",
        "    results = response.get('results', [])\n",
        "\n",
        "    if not results:\n",
        "        return \"No research content found.\"\n",
        "\n",
        "    research_summary = \"\\n\\n\".join([f\"{res['title']}: {res['url']}\" for res in results])\n",
        "    return research_summary\n",
        "\n",
        "# Define the drafting agent function\n",
        "def drafting_agent(research_content: Optional[str], query: str) -> str:\n",
        "    if research_content:\n",
        "        return f\"Here is the research for '{query}':\\n\\n{research_content}\"\n",
        "    return \"No research content available.\"\n",
        "\n",
        "# Define the search node\n",
        "def search_node(state: ResearchState) -> ResearchState:\n",
        "    research_content = research_agent(state[\"query\"])\n",
        "    state[\"research_content\"] = research_content\n",
        "    return state\n",
        "\n",
        "# Define the draft node\n",
        "def draft_node(state: ResearchState) -> ResearchState:\n",
        "    final_answer = drafting_agent(state[\"research_content\"], state[\"query\"])\n",
        "    state[\"final_answer\"] = final_answer\n",
        "    return state\n",
        "\n",
        "# Add nodes to the graph\n",
        "graph.add_node(\"search\", search_node)\n",
        "graph.add_node(\"draft\", draft_node)\n",
        "\n",
        "# Set entry point and edges\n",
        "graph.set_entry_point(\"search\")\n",
        "graph.add_edge(\"search\", \"draft\")\n",
        "\n",
        "# Compile the system\n",
        "deep_research_system = graph.compile()\n",
        "\n",
        "# Run it!\n",
        "previous_queries = []  # Initialize empty memory\n",
        "user_query = input(\"Enter your research query: \")\n",
        "state = {\"query\": user_query, \"research_content\": None, \"final_answer\": None}\n",
        "\n",
        "try:\n",
        "    final_state = deep_research_system.invoke(state)\n",
        "    previous_queries.append(user_query)  # Save query into memory\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error occurred: {e}\")\n",
        "    exit()  # or ask user to try again\n",
        "\n",
        "print(\"\\n\\nFinal Answer:\\n\")\n",
        "if final_state[\"final_answer\"]:\n",
        "    print(final_state[\"final_answer\"])\n",
        "else:\n",
        "    print(\"No result found.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "PsOcNXBlIOO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Experimenting by using different Techniques"
      ],
      "metadata": {
        "id": "7Z_6S5_jQkdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Not the Main Code, from here on (Optional)"
      ],
      "metadata": {
        "id": "2nWHMDaQQXRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers\n"
      ],
      "metadata": {
        "id": "5bNRfSrIwYlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "\n",
        "# Load a pipeline\n",
        "pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-large\", max_length=512)\n",
        "\n",
        "# Use LangChain wrapper\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n"
      ],
      "metadata": {
        "id": "O75j27_ewbd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connect Both Agents Using LangGraph\n",
        "\n"
      ],
      "metadata": {
        "id": "p92ZBrfJviLA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import langgraph\n",
        "\n",
        "# Define the steps\n",
        "def deep_research_system(user_query):\n",
        "    research_info = researcher_agent(user_query)\n",
        "    final_answer = answer_drafter_agent(research_info)\n",
        "    return final_answer\n"
      ],
      "metadata": {
        "id": "tTP9dZYGt_UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User Interaction"
      ],
      "metadata": {
        "id": "lIl9vQcKvzju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = input(\"Enter your research query: \")\n",
        "\n",
        "answer = deep_research_system(user_question)\n",
        "\n",
        "print(\"\\n\\nFinal Answer:\\n\")\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "tyP-H4tmvhm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deep_research_system(query):\n",
        "    # Fetch research data from search agent\n",
        "    search_data = search_agent.run(query)\n",
        "\n",
        "    # If search_data is a list, join it into a string\n",
        "    if isinstance(search_data, list):\n",
        "        search_data = \" \".join([str(item) for item in search_data])\n",
        "\n",
        "    # If it's a dictionary, extract the relevant content (assumes 'text' is the key holding the data)\n",
        "    if isinstance(search_data, dict):\n",
        "        search_data = search_data.get(\"text\", \"\")\n",
        "\n",
        "    # Now, clean and trim the research data\n",
        "    trimmed_data = trim_research_data(search_data)\n",
        "\n",
        "    # Prepare the final prompt\n",
        "    final_prompt = f\"Based on the following research data, draft a professional and concise answer:\\n\\n{trimmed_data}\\n\\nAnswer:\"\n",
        "\n",
        "    # Use the model to generate the answer\n",
        "    response = llm.invoke(final_prompt)\n",
        "\n",
        "    return response\n",
        "\n",
        "def trim_research_data(text, max_words=450):\n",
        "    words = text.split()\n",
        "    if len(words) > max_words:\n",
        "        words = words[:max_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Example query to test the system\n",
        "user_question = input(\"Enter your research query: \")\n",
        "answer = deep_research_system(user_question)\n",
        "\n",
        "print(\"\\n\\nFinal Answer:\\n\")\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "aMiJpK4n09jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deep_research_system(query):\n",
        "    try:\n",
        "        # Perform the research task and fetch data\n",
        "        research_data = search_agent.run(query)\n",
        "\n",
        "        # Check if the result is a list, and join it into a string if necessary\n",
        "        if isinstance(research_data, list):\n",
        "            research_data = \" \".join([str(item) for item in research_data])\n",
        "\n",
        "        # Trim research data to prevent overloading the model with too much information\n",
        "        trimmed_data = trim_research_data(research_data)\n",
        "\n",
        "        # Prepare the final prompt for the model\n",
        "        final_prompt = f\"Based on the following research data, draft a professional and concise answer:\\n\\n{trimmed_data}\\n\\nAnswer:\"\n",
        "\n",
        "        # Query the model for an answer\n",
        "        response = llm.invoke(final_prompt)\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error occurred: {str(e)}\"\n",
        "\n",
        "# Running the system with a query\n",
        "user_query = input(\"Enter your research query: \")\n",
        "answer = deep_research_system(user_query)\n",
        "\n",
        "print(\"\\n\\nFinal Answer:\\n\")\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "t8L-5QvU2g_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deep_research_system(query):\n",
        "    try:\n",
        "        # Perform the research task and fetch data\n",
        "        research_data = search_agent.run(query)\n",
        "\n",
        "        # If the result is a list, we will attempt to extract the most relevant information\n",
        "        if isinstance(research_data, list):\n",
        "            relevant_data = \"\"\n",
        "            for item in research_data:\n",
        "                if isinstance(item, dict):\n",
        "                    # Extract relevant content from 'content' or similar fields\n",
        "                    if 'content' in item:\n",
        "                        relevant_data += item['content'] + \"\\n\"\n",
        "                else:\n",
        "                    relevant_data += str(item) + \"\\n\"\n",
        "            research_data = relevant_data\n",
        "\n",
        "        # Trim the research data to avoid overloading the model\n",
        "        trimmed_data = trim_research_data(research_data)\n",
        "\n",
        "        # Prepare the final prompt to query the model\n",
        "        final_prompt = f\"Based on the following research data, provide a concise and relevant answer:\\n\\n{trimmed_data}\\n\\nAnswer (only date and day of the week):\"\n",
        "\n",
        "        # Query the model for a concise answer\n",
        "        response = llm.invoke(final_prompt)\n",
        "\n",
        "        return response.strip()\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error occurred: {str(e)}\"\n",
        "\n",
        "# Running the system with the query\n",
        "user_query = input(\"Enter your research query: \")\n",
        "answer = deep_research_system(user_query)\n",
        "\n",
        "print(\"\\n\\nFinal Answer:\\n\")\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "I2HUNY0T3tH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "def deep_research_system(query):\n",
        "    try:\n",
        "        # Get the current date and time in UTC, then convert to IST\n",
        "        utc_now = datetime.now(pytz.utc)\n",
        "        ist = pytz.timezone('Asia/Kolkata')\n",
        "        ist_now = utc_now.astimezone(ist)\n",
        "\n",
        "        # Format the response to only include the day and date in IST\n",
        "        day_of_week = ist_now.strftime('%A')  # Get the day of the week (e.g., Monday, Tuesday)\n",
        "        date_today = ist_now.strftime('%d %B %Y')  # Format the date (e.g., 25 April 2025)\n",
        "\n",
        "        # Construct the final, concise answer\n",
        "        final_answer = f\"Today is {day_of_week}, {date_today} in IST.\"\n",
        "\n",
        "        return final_answer\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error occurred: {str(e)}\"\n",
        "\n",
        "# Running the system with the query\n",
        "user_query = input(\"Enter your research query: \")\n",
        "answer = deep_research_system(user_query)\n",
        "\n",
        "print(\"\\n\\nFinal Answer:\\n\")\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "SYB5eDLo4Mk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Mini Research System using Facts + Wikipedia + DuckDuckGo (Optional)"
      ],
      "metadata": {
        "id": "Zv9gfPNgLUX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia\n"
      ],
      "metadata": {
        "id": "gnLqhd2H5dyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary package\n",
        "# pip install wikipedia\n",
        "\n",
        "import wikipedia\n",
        "\n",
        "def basic_nlp_preprocess(query):\n",
        "    # Very simple preprocessing: make lowercase, strip spaces\n",
        "    return query.lower().strip()\n",
        "\n",
        "def answer_query_from_wikipedia(query):\n",
        "    try:\n",
        "        processed_query = basic_nlp_preprocess(query)\n",
        "        summary = wikipedia.summary(processed_query, sentences=3)\n",
        "        return summary\n",
        "    except wikipedia.exceptions.DisambiguationError as e:\n",
        "        return f\"Your question is a bit broad. Try being more specific. Some options: {e.options}\"\n",
        "    except wikipedia.exceptions.PageError:\n",
        "        return \"Sorry, I couldn't find any information about that topic.\"\n",
        "    except Exception as e:\n",
        "        return f\"An unexpected error occurred: {str(e)}\"\n",
        "\n",
        "def main():\n",
        "    print(\"✨ Welcome to the Smart Research System ✨\\n\")\n",
        "    user_query = input(\"Enter your research query: \")\n",
        "    answer = answer_query_from_wikipedia(user_query)\n",
        "    print(\"\\n🔎 Final Answer:\\n\")\n",
        "    print(answer)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "YasYor9k5W1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia duckduckgo-search\n"
      ],
      "metadata": {
        "id": "YzKhzKVkCZ1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wikipedia\n",
        "from duckduckgo_search import DDGS\n",
        "\n",
        "# Simple hardcoded facts\n",
        "FACTS = {\n",
        "    \"how many days are there in a week\": \"There are 7 days in a week.\",\n",
        "    \"what is the color of sunflower\": \"The color of a sunflower is bright yellow.\",\n",
        "    \"what is the capital of india\": \"The capital of India is New Delhi.\",\n",
        "    \"how many minutes are there in an hour\": \"There are 60 minutes in an hour.\",\n",
        "    # Add more if you want\n",
        "}\n",
        "\n",
        "def basic_nlp_preprocess(query):\n",
        "    return query.lower().strip()\n",
        "\n",
        "def check_facts_database(query):\n",
        "    return FACTS.get(query)\n",
        "\n",
        "def fetch_from_wikipedia(query):\n",
        "    try:\n",
        "        summary = wikipedia.summary(query, sentences=2)\n",
        "        return summary\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def fetch_from_duckduckgo(query):\n",
        "    try:\n",
        "        ddgs = DDGS()\n",
        "        results = ddgs.text(query, max_results=3)\n",
        "        if results:\n",
        "            return results[0]['body']\n",
        "        else:\n",
        "            return None\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "def deep_research_system(query):\n",
        "    query = basic_nlp_preprocess(query)\n",
        "\n",
        "    # 1. Check hardcoded facts\n",
        "    answer = check_facts_database(query)\n",
        "    if answer:\n",
        "        return answer\n",
        "\n",
        "    # 2. Try Wikipedia\n",
        "    answer = fetch_from_wikipedia(query)\n",
        "    if answer:\n",
        "        return answer\n",
        "\n",
        "    # 3. Finally, try DuckDuckGo\n",
        "    answer = fetch_from_duckduckgo(query)\n",
        "    if answer:\n",
        "        return answer\n",
        "\n",
        "    return \"Sorry, I couldn't find an answer to your question.\"\n",
        "\n",
        "# ----------- Test it -------------\n",
        "user_question = input(\"Enter your research query: \")\n",
        "answer = deep_research_system(user_question)\n",
        "print(\"\\n\\nFinal Answer:\\n\")\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "hiEbqLwrCfOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nbformat\n",
        "\n",
        "# Load the notebook\n",
        "with open('AI_Agent.ipynb', 'r') as file:\n",
        "    notebook = nbformat.read(file, as_version=4)\n",
        "\n",
        "# Add missing 'state' in widgets metadata if it doesn't exist\n",
        "if 'widgets' in notebook.metadata:\n",
        "    if 'state' not in notebook.metadata['widgets']:\n",
        "        notebook.metadata['widgets']['state'] = {}\n",
        "\n",
        "# Save the modified notebook\n",
        "with open('fixed_notebook.ipynb', 'w') as file:\n",
        "    nbformat.write(notebook, file)\n"
      ],
      "metadata": {
        "id": "U38aFjruSlei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(\"Current working directory:\", os.getcwd())\n"
      ],
      "metadata": {
        "id": "NB8CraBiTSvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Now you can access the file from your Google Drive path\n",
        "with open('/content/drive/MyDrive/your_folder_name/AI_Agent.ipynb', 'r') as file:\n",
        "    notebook = nbformat.read(file, as_version=4)\n"
      ],
      "metadata": {
        "id": "OwY0ekC7UrTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "zFRnmZwdVNSQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}